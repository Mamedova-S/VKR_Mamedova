{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t chesterish -T -N -kl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import Tfidata_randomVectorizer\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_random=pd.read_csv('C:/projects/VKR_Mamedova/datasets/training.csv',encoding='ISO-8859-1',header=None)\n",
    "data_depressive= pd.read_csv('C:/projects/VKR_Mamedova/datasets/Suicide_Detection.csv')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "def load_google_vec(self):\n",
    "    #url = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "    #wget.download(url, 'Data/GoogleNews-vectors.bin.gz')\n",
    "    return KeyedVectors.load_word2vec_format(\n",
    "        'Data/GoogleNews-vectors.bin.gz',\n",
    "        binary=True)\n",
    "\n",
    "\n",
    "# Download link\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3  \\\n",
       "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                       4                                                  5  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>I’m so lostHello, my name is Adam (16) and I’v...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232069</th>\n",
       "      <td>348103</td>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232070</th>\n",
       "      <td>348106</td>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232071</th>\n",
       "      <td>348107</td>\n",
       "      <td>pee probably tastes like salty tea😏💦‼️ can som...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232072</th>\n",
       "      <td>348108</td>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232073</th>\n",
       "      <td>348110</td>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232074 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  \\\n",
       "0                2  Ex Wife Threatening SuicideRecently I left my ...   \n",
       "1                3  Am I weird I don't get affected by compliments...   \n",
       "2                4  Finally 2020 is almost over... So I can never ...   \n",
       "3                8          i need helpjust help me im crying so hard   \n",
       "4                9  I’m so lostHello, my name is Adam (16) and I’v...   \n",
       "...            ...                                                ...   \n",
       "232069      348103  If you don't like rock then your not going to ...   \n",
       "232070      348106  You how you can tell i have so many friends an...   \n",
       "232071      348107  pee probably tastes like salty tea😏💦‼️ can som...   \n",
       "232072      348108  The usual stuff you find hereI'm not posting t...   \n",
       "232073      348110  I still haven't beaten the first boss in Hollo...   \n",
       "\n",
       "              class  \n",
       "0           suicide  \n",
       "1       non-suicide  \n",
       "2       non-suicide  \n",
       "3           suicide  \n",
       "4           suicide  \n",
       "...             ...  \n",
       "232069  non-suicide  \n",
       "232070  non-suicide  \n",
       "232071  non-suicide  \n",
       "232072      suicide  \n",
       "232073  non-suicide  \n",
       "\n",
       "[232074 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_depressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5], dtype='int64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=data_random.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  5\n",
       "0  0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1  0  is upset that he can't update his Facebook by ...\n",
       "2  0  @Kenichan I dived many times for the ball. Man...\n",
       "3  0    my whole body feels itchy and like its on fire \n",
       "4  0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_random.drop([1,2,3,4],axis=1,inplace=True)\n",
    "data_random.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      0  is upset that he can't update his Facebook by ...\n",
       "2      0  @Kenichan I dived many times for the ball. Man...\n",
       "3      0    my whole body feels itchy and like its on fire \n",
       "4      0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_random.columns=['Label','Text']\n",
    "data_random.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\d+)|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww that s a bummer You shoulda got David Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can t update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I dived many times for the ball Managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no it s not behaving at all i m mad why am i h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up Having no school is the best feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB com Very cool to hear old Walt interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover Ask me fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy th Birthday to my boo of alll time Tupac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>happy charitytuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label                                               Text\n",
       "0            0  Awww that s a bummer You shoulda got David Car...\n",
       "1            0  is upset that he can t update his Facebook by ...\n",
       "2            0  I dived many times for the ball Managed to sav...\n",
       "3            0     my whole body feels itchy and like its on fire\n",
       "4            0  no it s not behaving at all i m mad why am i h...\n",
       "...        ...                                                ...\n",
       "1599995      4  Just woke up Having no school is the best feel...\n",
       "1599996      4   TheWDB com Very cool to hear old Walt interviews\n",
       "1599997      4  Are you ready for your MoJo Makeover Ask me fo...\n",
       "1599998      4  Happy th Birthday to my boo of alll time Tupac...\n",
       "1599999      4                               happy charitytuesday\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_random['Text'] = data_random['Text'].apply(clean_tweet)\n",
    "data_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предварительная обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация\n",
    "Токенизация — процесс разбиения текстового документа на отдельные слова, которые называются токенами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(sen) for sen in data_random.Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_random['Text_Final'] = [' '.join(sen) for sen in lower_tokens]\n",
    "data_random['tokens'] = lower_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс конвертации текста в числа называется векторизацией. Теперь после Text Preprocessing, нам нужно представить текст в числовом виде, то есть закодировать текстовые данные в виде чисел, которые в дальнейшем могут использоваться в алгоритмах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- CLASSIFICATION ALGORITHMS -----------------------------------------------------------------------------------------\n",
    "\n",
    "def LR_classification(file_store_dir, trainX, trainY, testX, testY, feats):\n",
    "    # among several small values, 0.001 was the one optimizing the results - quicker convergence, higher scores\n",
    "    C=5e-5\n",
    "    # scaling=\"Standard\"\n",
    "    # LR = make_pipeline(StandardScaler(),LogisticRegression(C=C, solver=\"lbfgs\", max_iter=1000 ))\n",
    "    # scaling=\"Normal\"\n",
    "    # LR = make_pipeline(MinMaxScaler(),LogisticRegression(C=C, solver=\"lbfgs\", max_iter=1000 ))\n",
    "    scaling=\"noScaling\"\n",
    "    LR = LogisticRegression(C=C, solver=\"lbfgs\", max_iter=1000 )\n",
    "    LR.fit(trainX, trainY)\n",
    "    LR_scores_precision = evaluate_model_precision(LR, testX, testY)\n",
    "    LR_scores_recall = evaluate_model_recall(LR, testX, testY)\n",
    "    LR_scores_f1 = evaluate_model_f1(LR, testX, testY)\n",
    "\n",
    "    scores = f'\\n> Logistic Regression (C: {C})\\nmean F1: {mean(LR_scores_f1):.6f} | mean Precision: {mean(LR_scores_precision):.6f} | mean Recall: {mean(LR_scores_recall):.6f}\\nstd F1: {std(LR_scores_f1):.6f} | std Precision: {std(LR_scores_precision):.6f} | std Recall: {std(LR_scores_recall):.6f}'\n",
    "\n",
    "    y_pred = LR.predict(testX)\n",
    "    log_str = scores+\"\\n\\n\"+classification_report(testY, y_pred)+\"\\n\\n\"+str(LR.get_params())\n",
    "    print(log_str)\n",
    "\n",
    "    log_dir = os.path.join(file_store_dir,\"LogisticRegression\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    log_path = os.path.join(log_dir,f\"LR_{scaling}_{feats}.txt\")\n",
    "    with open(log_path,\"w\",encoding=\"utf-8\") as log_writer:\n",
    "        log_writer.write(log_str)\n",
    "\n",
    "    return LR\n",
    "\n",
    "def KNN_classification(file_store_dir, trainX, trainY, testX, testY, feats):\n",
    "    # among several values, 5 was the one optimizing the results\n",
    "    k_n=5\n",
    "    # scaling=\"Standard\"\n",
    "    # KNN = make_pipeline(StandardScaler(),KNeighborsClassifier(n_neighbors=k_n))\n",
    "    # scaling=\"Normal\"\n",
    "    # KNN = make_pipeline(MinMaxScaler(),KNeighborsClassifier(n_neighbors=k_n))\n",
    "    scaling=\"noScaling\"\n",
    "    KNN = KNeighborsClassifier(n_neighbors=k_n)\n",
    "    KNN.fit(trainX, trainY)\n",
    "    KNN_scores_precision = evaluate_model_precision(KNN, testX, testY)\n",
    "    KNN_scores_recall = evaluate_model_recall(KNN, testX, testY)\n",
    "    KNN_scores_f1 = evaluate_model_f1(KNN, testX, testY)\n",
    "\n",
    "    scores = f'\\n> k Nearest Neighbor (n_neighbors:{k_n}) \\nmean F1: {mean(KNN_scores_f1):.6f} | mean Precision: {mean(KNN_scores_precision):.6f} | mean Recall: {mean(KNN_scores_recall):.6f}\\nstd F1: {std(KNN_scores_f1):.6f} | std Precision: {std(KNN_scores_precision):.6f} | std Recall: {std(KNN_scores_recall):.6f}'\n",
    "\n",
    "    y_pred = KNN.predict(testX)\n",
    "    log_str = scores+\"\\n\\n\"+classification_report(testY, y_pred)+\"\\n\\n\"+str(KNN.get_params())\n",
    "    print(log_str)\n",
    "\n",
    "    log_dir = os.path.join(file_store_dir,\"KNearestNeighbors\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    log_path = os.path.join(log_dir,f\"KNN_{scaling}_{feats}.txt\")\n",
    "    with open(log_path,\"w\",encoding=\"utf-8\") as log_writer:\n",
    "        log_writer.write(log_str)\n",
    "\n",
    "    return KNN\n",
    "\n",
    "def SVM_classification(file_store_dir, trainX, trainY, testX, testY, feats):    \n",
    "    C=1.0\n",
    "    kernel=\"rbf\"\n",
    "    # scaling=\"Standard\"\n",
    "    # SVM = make_pipeline(StandardScaler(),SVC(C=C, kernel=kernel))\n",
    "    # scaling=\"Normal\"\n",
    "    # SVM = make_pipeline(MinMaxScaler(),SVC(C=C, kernel=kernel))\n",
    "    scaling=\"NoScaling\"\n",
    "    SVM = SVC(C=C, kernel=kernel,probability=True)\n",
    "    SVM.fit(trainX, trainY)\n",
    "    SVM_scores_precision = evaluate_model_precision(SVM, testX, testY)\n",
    "    SVM_scores_recall = evaluate_model_recall(SVM, testX, testY)\n",
    "    SVM_scores_f1 = evaluate_model_f1(SVM, testX, testY)\n",
    "\n",
    "    scores = f'\\n> Support Vector Machine (C: {C}) \\nmean F1: {mean(SVM_scores_f1):.6f} | mean Precision: {mean(SVM_scores_precision):.6f} | mean Recall: {mean(SVM_scores_recall):.6f}\\nstd F1: {std(SVM_scores_f1):.6f} | std Precision: {std(SVM_scores_precision):.6f} | std Recall: {std(SVM_scores_recall):.6f}'\n",
    "\n",
    "    y_pred = SVM.predict(testX)\n",
    "    log_str = scores+\"\\n\\n\"+classification_report(testY, y_pred)+\"\\n\\n\"+str(SVM.get_params())\n",
    "    print(log_str)\n",
    "\n",
    "    log_dir = os.path.join(file_store_dir,\"SupportVectorMachine\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    log_path = os.path.join(log_dir,f\"SVM_{scaling}_{feats}.txt\")\n",
    "    with open(log_path,\"w\",encoding=\"utf-8\") as log_writer:\n",
    "        log_writer.write(log_str)\n",
    "\n",
    "    return SVM\n",
    "\n",
    "def RF_classification(file_store_dir, trainX, trainY, testX, testY, feats):\n",
    "    n_estimators = 100\n",
    "    scaling=\"Standard\"\n",
    "    RF = make_pipeline(StandardScaler(),RandomForestClassifier(n_estimators=n_estimators))\n",
    "    # scaling=\"Normal\"\n",
    "    # RF = make_pipeline(MinMaxScaler(),RandomForestClassifier(n_estimators=n_estimators))\n",
    "    # scaling=\"NoScaling\"\n",
    "    # RF = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    RF.fit(trainX, trainY)\n",
    "    RF_scores_precision = evaluate_model_precision(RF, testX, testY)\n",
    "    RF_scores_recall = evaluate_model_recall(RF, testX, testY)\n",
    "    RF_scores_f1 = evaluate_model_f1(RF, testX, testY)\n",
    "\n",
    "    scores = f'\\n> Random Forest \\nmean F1: {mean(RF_scores_f1):.6f} | mean Precision: {mean(RF_scores_precision):.6f} | mean Recall: {mean(RF_scores_recall):.6f}\\nstd F1: {std(RF_scores_f1):.6f} | std Precision: {std(RF_scores_precision):.6f} | std Recall: {std(RF_scores_recall):.6f}'\n",
    "\n",
    "    y_pred = RF.predict(testX)\n",
    "    log_str = scores+\"\\n\\n\"+classification_report(testY, y_pred)+\"\\n\\n\"+str(RF.get_params())\n",
    "    print(log_str)\n",
    "\n",
    "    log_dir = os.path.join(file_store_dir,\"RandomForest\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    log_path = os.path.join(log_dir,f\"RF_{scaling}_{feats}.txt\")\n",
    "    with open(log_path,\"w\",encoding=\"utf-8\") as log_writer:\n",
    "        log_writer.write(log_str)\n",
    "        \n",
    "    return RF\n",
    "\n",
    "\n",
    "\n",
    "#---------- ARTIFICIAL NEURAL NETWORKS -----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def model_training(model,loss_func,opt,trainX,trainY,batch_size,epochs,testX,testY, csv_logger, metrics):\n",
    "    \n",
    "    model.compile(loss=loss_func, optimizer=opt, metrics=metrics)\n",
    "    history = model.fit(trainX, trainY, batch_size=batch_size, epochs=epochs, validation_data=(testX, testY), verbose=1, shuffle=True, callbacks=[csv_logger])\n",
    "    \n",
    "    return model, history\n",
    "    \n",
    "# model 1\n",
    "def model_biLSTM(dropout2_drp=0.2, dense2_num=8, lstm1_num=32, lstm2_num=16, dropout_lstm=0.2):\n",
    "    kernel_reg=regularizers.l1_l2()\n",
    "#     kernel_reg=None\n",
    "    dense1_num=32\n",
    "    dense1_activation=\"relu\"\n",
    "\n",
    "    dropout1_drp=0.9\n",
    "    \n",
    "    dense2_activation=\"relu\"\n",
    "    \n",
    "    classes_num=1\n",
    "    final_activation=\"sigmoid\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( Dense( dense1_num, activation=dense1_activation, kernel_regularizer=kernel_reg) )\n",
    "    model.add( Bidirectional(LSTM(lstm1_num, dropout = dropout_lstm, return_sequences=True,kernel_regularizer=kernel_reg  ) ) )\n",
    "#     model.add( Bidirectional(LSTM(lstm2_num)) )\n",
    "    model.add( Dense(dense2_num, activation=dense2_activation) )\n",
    "    model.add( Dropout(dropout2_drp) )\n",
    "    model.add( Dense(classes_num, activation=final_activation) )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# model 2\n",
    "def model_CNN(f1=16, k1_size=2, f2=8, k2_size=2, dropout2_drp=0.15, dense2_num=8):\n",
    "    dense2_activation=\"relu\"\n",
    "#     dropout2_drp=0.2\n",
    "    \n",
    "    classes_num=1\n",
    "    final_activation=\"sigmoid\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=f1, kernel_size=k1_size, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=f2, kernel_size=k2_size, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense2_num, activation='relu'))\n",
    "    model.add( Dropout(dropout2_drp) )\n",
    "    model.add( Dense(classes_num, activation=final_activation) )\n",
    "\n",
    "    return model\n",
    "\n",
    "# model 3\n",
    "def model_BiGRU():\n",
    "    dense2_num=8\n",
    "    dense2_activation=\"relu\"\n",
    "    dropout2_drp=0.2\n",
    "    \n",
    "    gru_units = 32\n",
    "    \n",
    "    classes_num=1\n",
    "    final_activation=\"sigmoid\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( Bidirectional(CuDNNGRU(gru_units, return_sequences=True)))\n",
    "    model.add( Dropout(0.2))\n",
    "    model.add( Dense(gru_units*2, activation='relu'))\n",
    "    model.add( Dropout(0.1))\n",
    "    model.add( Dense(gru_units, activation='relu'))\n",
    "    model.add( Dropout(dropout2_drp) )\n",
    "    model.add( Dense(classes_num, activation=final_activation) )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# model 1\n",
    "def BiLSTM_classification(file_store_dir, trainX_orig, trainY_orig, testX_orig, testY_orig, epochs, feats):\n",
    "    model_name=\"modelBiLSTM\"\n",
    "    model_path=os.path.join(file_store_dir,f\"{model_name}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    classes_num=1\n",
    "    # data set reshape\n",
    "    trainX = trainX_orig.reshape(-1, 1, trainX_orig.shape[1])\n",
    "    testX = testX_orig.reshape(-1, 1, testX_orig.shape[1])\n",
    "\n",
    "    # trainY = categorical_encoding(trainY_allFeats)\n",
    "    # trainY = trainY.reshape(-1, 1, classes_num)\n",
    "    trainY = trainY_orig.reshape(-1, 1, classes_num)\n",
    "\n",
    "    # testY= categorical_encoding(testY_allFeats)\n",
    "    # testY= testY.reshape(-1, 1, classes_num)\n",
    "    testY = testY_orig.reshape(-1, 1, classes_num)\n",
    "\n",
    "    loss_fun = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    lr = 5e-5\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    # opt = RMSprop(learning_rate=lr)\n",
    "    batch_size = 24\n",
    "#     epochs = 20\n",
    "\n",
    "    csv_logger = CSVLogger(os.path.join(model_path,f'log_{model_name}_{feats}.csv'), separator=',' )\n",
    "    metrics= [ \"accuracy\", tfa.metrics.F1Score(num_classes=2, average=\"micro\", threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall() ]\n",
    "    # metrics = [\"accuracy\",f1_s,precision_s,recall_s]\n",
    "    model_1 = model_biLSTM()\n",
    "    print(f\"\\n\\nBiLSTM\\n\")\n",
    "    model_1, history_1 = model_training(model_1,loss_fun,opt,trainX,trainY,batch_size,epochs,testX,testY, csv_logger, metrics)\n",
    "\n",
    "    loss, accuracy, f1_score, precision, recall = model_1.evaluate(trainX, trainY, verbose=0)\n",
    "    eval_str_training = f\"\\nBiLSTM\\n\\nTraining Set\\nLoss: {loss: .6f}\\nAccuracy: {accuracy:.6f}\\nPrecision: {precision:.6f}\\nRecall: {recall:.6f}\\nF1: {f1_score:.6f}\\n\\n\"\n",
    "    loss, accuracy, f1_score, precision, recall = model_1.evaluate(testX, testY, verbose=0)\n",
    "    eval_str_test = f\"\\n\\nTest Set\\nLoss: {loss: .6f}\\nAccuracy: {accuracy:.6f}\\nPrecision: {precision:.6f}\\nRecall: {recall:.6f}\\nF1: {f1_score:.6f}\\n\\n\"\n",
    "    # print(\"\\n\\n\"+eval_str_training, eval_str_test)\n",
    "\n",
    "#     plot_history(history_1, model_path,feats, \"BiLSTM\")\n",
    "\n",
    "    with open(os.path.join(model_path,f\"{model_name}_evaluationScore_{feats}.txt\"),\"w\",encoding=\"utf-8\") as evaluation_file:\n",
    "        evaluation_file.write(class_report(model_1,trainX,trainY,testX,testY))\n",
    "        evaluation_file.write(\"\\n\\n\"+eval_str_training+eval_str_test)\n",
    "\n",
    "#     from IPython.display import FileLink, FileLinks\n",
    "#     FileLinks(\".\")\n",
    "\n",
    "    print(f\"\\n{class_report(model_1,trainX,trainY,testX,testY)}\" )\n",
    "    \n",
    "    return model_1, history_1\n",
    "\n",
    "## model 2\n",
    "def CNN_classification(file_store_dir, trainX_orig, trainY_orig, testX_orig, testY_orig, epochs, feats):\n",
    "    model_name=\"modelCNN\"\n",
    "    model_path=os.path.join(file_store_dir,f\"{model_name}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    classes_num=1\n",
    "    # data set reshape\n",
    "    trainX = trainX_orig.reshape(trainX_orig.shape[0], trainX_orig.shape[1], -1)\n",
    "    testX = testX_orig.reshape(testX_orig.shape[0], testX_orig.shape[1], -1)\n",
    "\n",
    "    # le = LabelEncoder()\n",
    "    # trainY = categorical_encoding(trainY_allFeats)\n",
    "    # trainY = trainY.reshape(-1, 1, classes_num)\n",
    "    trainY = trainY_orig.reshape(-1, classes_num)\n",
    "    # trainY = le.fit_transform()\n",
    "\n",
    "    # testY= categorical_encoding(testY_allFeats)\n",
    "    # testY= testY.reshape(-1, 1, classes_num)\n",
    "    testY = testY_orig.reshape(-1, classes_num)\n",
    "\n",
    "    loss_fun = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    lr = 5e-5\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    # opt = RMSprop(learning_rate=lr)\n",
    "    batch_size = 24\n",
    "#     epochs = 20\n",
    "\n",
    "    csv_logger = CSVLogger(os.path.join(model_path,f'log_{model_name}_{feats}.csv'), separator=',')\n",
    "    metrics= [ \"accuracy\", tfa.metrics.F1Score(num_classes=2, average=\"micro\", threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall() ]\n",
    "\n",
    "    model_2= model_CNN()\n",
    "    print(f\"\\n\\nCNN\\n\")\n",
    "    model_2,history_2 = model_training(model_2,loss_fun,opt,trainX,trainY,batch_size,epochs,testX,testY, csv_logger, metrics)\n",
    "\n",
    "    loss, accuracy, f1_score, precision, recall = model_2.evaluate(trainX, trainY, verbose=0)\n",
    "    eval_str_training = f\"\\nCNN\\n\\nTraining Set\\nLoss: {loss: .6f}\\nAccuracy: {accuracy:.6f}\\nPrecision: {precision:.6f}\\nRecall: {recall:.6f}\\nF1: {f1_score:.6f}\\n\\n\"\n",
    "    loss, accuracy, f1_score, precision, recall = model_2.evaluate(testX, testY, verbose=0)\n",
    "    eval_str_test = f\"\\n\\nTest Set\\nLoss: {loss: .6f}\\nAccuracy: {accuracy:.6f}\\nPrecision: {precision:.6f}\\nRecall: {recall:.6f}\\nF1: {f1_score:.6f}\\n\\n\"\n",
    "    # print(\"\\n\\n\"+eval_str_training, eval_str_test)\n",
    "\n",
    "#     plot_history(history_2, model_path, feats, \"CNN\")\n",
    "\n",
    "    with open(os.path.join(model_path,f\"{model_name}_evaluationScore_{feats}.txt\"),\"w\",encoding=\"utf-8\") as evaluation_file:\n",
    "        evaluation_file.write(class_report(model_2,trainX,trainY,testX,testY))\n",
    "        evaluation_file.write(\"\\n\\n\"+eval_str_training+eval_str_test)\n",
    "\n",
    "#     from IPython.display import FileLink, FileLinks\n",
    "#     FileLinks(\".\")   \n",
    "    print(f\"\\n{class_report(model_2,trainX,trainY,testX,testY)}\" )\n",
    "    \n",
    "    return model_2, history_2\n",
    "\n",
    "## model 3\n",
    "def BiGRU_classification(file_store_dir, trainX_orig, trainY_orig, testX_orig, testY_orig, epochs, feats):\n",
    "    model_name=\"modelBiGRU\"\n",
    "    model_path=os.path.join(file_store_dir,f\"{model_name}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    classes_num=1\n",
    "    # # data set reshape\n",
    "    trainX = trainX_orig.reshape(-1, 1, trainX_orig.shape[1])\n",
    "    testX = testX_orig.reshape(-1, 1, testX_orig.shape[1])\n",
    "\n",
    "    # trainY = categorical_encoding(trainY_allFeats)\n",
    "    # trainY = trainY.reshape(-1, 1, classes_num)\n",
    "    trainY = trainY_orig.reshape(-1, 1, classes_num)\n",
    "\n",
    "    # testY= categorical_encoding(testY_allFeats)\n",
    "    # testY= testY.reshape(-1, 1, classes_num)\n",
    "    testY = testY_orig.reshape(-1, 1, classes_num)\n",
    "\n",
    "    loss_fun = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    lr = 5e-5\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    # opt = RMSprop(learning_rate=lr)\n",
    "    batch_size = 24\n",
    "#     epochs = 20\n",
    "\n",
    "    csv_logger = CSVLogger(os.path.join(model_path,f'log_{model_name}_{feats}.csv'), separator=',')\n",
    "    metrics= [ \"accuracy\", tfa.metrics.F1Score(num_classes=2, average=\"micro\", threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall() ]\n",
    "\n",
    "    model_3= model_BiGRU()\n",
    "    print(f\"\\n\\nBiGRU\\n\")\n",
    "    model_3,history_3 = model_training(model_3,loss_fun,opt,trainX,trainY,batch_size,epochs,testX,testY, csv_logger, metrics)\n",
    "\n",
    "    loss, accuracy, f1_score, precision, recall = model_3.evaluate(trainX, trainY, verbose=0)\n",
    "    eval_str_training = f\"\\nBiGRU\\n\\nTraining Set\\nLoss: {loss: .6f}\\nAccuracy: {accuracy:.6f}\\nPrecision: {precision:.6f}\\nRecall: {recall:.6f}\\nF1: {f1_score:.6f}\\n\\n\"\n",
    "    loss, accuracy, f1_score, precision, recall = model_3.evaluate(testX, testY, verbose=0)\n",
    "    eval_str_test = f\"\\n\\nTest Set\\nLoss: {loss: .6f}\\nAccuracy: {accuracy:.6f}\\nPrecision: {precision:.6f}\\nRecall: {recall:.6f}\\nF1: {f1_score:.6f}\\n\\n\"\n",
    "    # print(\"\\n\\n\"+eval_str_training, eval_str_test)\n",
    "\n",
    "#     plot_history(history_3, model_path, feats, \"BiGRU\")\n",
    "\n",
    "    with open(os.path.join(model_path,f\"{model_name}_evaluationScore_{feats}.txt\"),\"w\",encoding=\"utf-8\") as evaluation_file:\n",
    "        evaluation_file.write(class_report(model_3,trainX,trainY,testX,testY))\n",
    "        evaluation_file.write(\"\\n\\n\"+eval_str_training+eval_str_test)\n",
    "    \n",
    "    print(f\"\\n{class_report(model_3,trainX,trainY,testX,testY)}\" )\n",
    "#     from IPython.display import FileLink, FileLinks\n",
    "#     FileLinks(\".\")   \n",
    "\n",
    "    return model_3, history_3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (800000) does not match length of index (1600000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m         pos\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m         neg\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m data_random[\u001b[39m'\u001b[39m\u001b[39mPos\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m pos\n\u001b[0;32m     11\u001b[0m data_random[\u001b[39m'\u001b[39m\u001b[39mNeg\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m neg\n\u001b[0;32m     13\u001b[0m data_random \u001b[39m=\u001b[39m data_random[[\u001b[39m'\u001b[39m\u001b[39mText_Final\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPos\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNeg\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\anana\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\anana\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4163\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4164\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4170\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   4174\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   4175\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   4177\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4178\u001b[0m     ):\n\u001b[0;32m   4179\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\anana\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:4912\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4909\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4911\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4912\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4913\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\anana\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (800000) does not match length of index (1600000)"
     ]
    }
   ],
   "source": [
    "\n",
    "pos=[]\n",
    "neg=[]\n",
    "for l in data_random.Label:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "data_random['Pos']= pos\n",
    "data_random['Neg']= neg\n",
    "\n",
    "data_random = data_random[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]\n",
    "data_random.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data_random,test_size=0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download link\n",
    "# https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'   \n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1,EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "     train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    " \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, \n",
    "                        kernel_size=filter_size, \n",
    "                        activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1,EMBEDDING_DIM, len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.3, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('depx007.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(tokenizer,'model_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "281bf0cb9ee4d571eff4c2f5d6b84ce0c0bfe1e1cb074b260d2fbf8de124818f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
